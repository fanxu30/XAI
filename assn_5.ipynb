{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8f3hSs5nFF2ML/BNJ+Vkb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fanxu30/XAI/blob/main/assn_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# AIPI 590 - XAI | Assignment #05 Explainable Deep Learning\n",
        "\n",
        "## Fan Xu\n",
        "# Model Explainability in Computer Vision: Wildlife Conservation Case Study\n",
        "\n",
        "### This notebook applies GradCAM and its variants to analyze model predictions for wildlife classification, specifically focusing on zebra detection in camera trap images.\n",
        "\n",
        "I certify that I did not use AI in this assignment"
      ],
      "metadata": {
        "id": "1mgzCgvFFgJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Setup and Imports\n",
        "\n",
        "# %%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import requests\n",
        "from PIL import Image\n",
        "import json\n",
        "from typing import List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRdQvMjyFwhM",
        "outputId": "fc98c9cd-c447-4bfa-de26-eeb3e238f577"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image preprocessing\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Inverse transform for visualization\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "    std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Download sample wildlife images\n",
        "def download_sample_images():\n",
        "    \"\"\"Download sample wildlife images for analysis\"\"\"\n",
        "    image_urls = [\n",
        "        # Zebra images\n",
        "        \"https://raw.githubusercontent.com/zhoubolei/CAM/master/example/zebra.jpg\",\n",
        "        \"https://images.unsplash.com/photo-1543946608-3b7c2b52b4e3?w=400\",\n",
        "        \"https://images.unsplash.com/photo-1598974357801-cbca100e65d3?w=400\",\n",
        "        # Other animals\n",
        "        \"https://images.unsplash.com/photo-1546182990-dffeafbe841d?w=400\",  # Lion\n",
        "        \"https://images.unsplash.com/photo-1564349683136-77e08dba1ef7?w=400\",  # Elephant\n",
        "        \"https://images.unsplash.com/photo-1557050543-4d5f4e07ef46?w=400\",  # Giraffe\n",
        "    ]\n",
        "\n",
        "    images = []\n",
        "    for i, url in enumerate(image_urls):\n",
        "        try:\n",
        "            response = requests.get(url, stream=True)\n",
        "            img = Image.open(response.raw).convert('RGB')\n",
        "            images.append(img)\n",
        "            print(f\"Downloaded image {i+1}\")\n",
        "        except:\n",
        "            # Fallback: create a simple colored image\n",
        "            img = Image.new('RGB', (224, 224), color=(73, 109, 137))\n",
        "            images.append(img)\n",
        "            print(f\"Created fallback image {i+1}\")\n",
        "\n",
        "    return images\n",
        "\n",
        "# Download and preprocess images\n",
        "sample_images = download_sample_images()\n",
        "preprocessed_images = [preprocess(img) for img in sample_images]\n",
        "image_tensors = torch.stack(preprocessed_images).to(device)\n",
        "\n",
        "print(f\"Loaded {len(sample_images)} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISxTbNXVGExc",
        "outputId": "bf03bd65-a0c6-4e14-8558-dee95e8bf2b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created fallback image 1\n",
            "Created fallback image 2\n",
            "Downloaded image 3\n",
            "Downloaded image 4\n",
            "Downloaded image 5\n",
            "Downloaded image 6\n",
            "Loaded 6 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained ResNet-50\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# ImageNet class labels\n",
        "with open('imagenet_class_index.json', 'r') as f:\n",
        "    class_idx = json.load(f)\n",
        "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
        "\n",
        "def get_prediction(model, image_tensor):\n",
        "    \"\"\"Get model prediction and confidence\"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor.unsqueeze(0))\n",
        "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "        confidence, predicted_idx = torch.max(probabilities, 0)\n",
        "        predicted_label = idx2label[predicted_idx.item()]\n",
        "    return predicted_label, confidence.item(), predicted_idx.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "ZElQwX2dGb3l",
        "outputId": "147ecb9c-8cd5-454c-e8d6-a9bbb0619537"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 88.8MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'imagenet_class_index.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2066939276.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# ImageNet class labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imagenet_class_index.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mclass_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0midx2label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'imagenet_class_index.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GradCAM:\n",
        "    \"\"\"GradCAM implementation for CNN models\"\"\"\n",
        "\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Register hooks\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients = grad_output[0]\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def generate_cam(self, input_tensor, target_class=None):\n",
        "        \"\"\"Generate GradCAM heatmap\"\"\"\n",
        "        # Forward pass\n",
        "        model_output = self.model(input_tensor.unsqueeze(0))\n",
        "\n",
        "        if target_class is None:\n",
        "            target_class = torch.argmax(model_output, dim=1)\n",
        "\n",
        "        # Zero gradients\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Backward pass for target class\n",
        "        one_hot_output = torch.zeros_like(model_output)\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        model_output.backward(gradient=one_hot_output)\n",
        "\n",
        "        # Get gradients and activations\n",
        "        gradients = self.gradients.cpu().data.numpy()[0]\n",
        "        activations = self.activations.cpu().data.numpy()[0]\n",
        "\n",
        "        # Global average pooling of gradients\n",
        "        weights = np.mean(gradients, axis=(1, 2))\n",
        "\n",
        "        # Weighted combination of activation maps\n",
        "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * activations[i]\n",
        "\n",
        "        # Apply ReLU\n",
        "        cam = np.maximum(cam, 0)\n",
        "\n",
        "        # Resize to input image size\n",
        "        cam = cv2.resize(cam, (224, 224))\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / (np.max(cam) + 1e-8)\n",
        "\n",
        "        return cam"
      ],
      "metadata": {
        "id": "iao-3J-MGh7-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradCAMPlusPlus(GradCAM):\n",
        "    \"\"\"GradCAM++ implementation - improved version of GradCAM\"\"\"\n",
        "\n",
        "    def generate_cam(self, input_tensor, target_class=None):\n",
        "        \"\"\"Generate GradCAM++ heatmap\"\"\"\n",
        "        # Forward pass\n",
        "        model_output = self.model(input_tensor.unsqueeze(0))\n",
        "\n",
        "        if target_class is None:\n",
        "            target_class = torch.argmax(model_output, dim=1)\n",
        "\n",
        "        # Zero gradients\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Backward pass for target class\n",
        "        one_hot_output = torch.zeros_like(model_output)\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        model_output.backward(gradient=one_hot_output)\n",
        "\n",
        "        # Get gradients and activations\n",
        "        gradients = self.gradients.cpu().data.numpy()[0]\n",
        "        activations = self.activations.cpu().data.numpy()[0]\n",
        "\n",
        "        # GradCAM++ specific calculations\n",
        "        numerator = gradients ** 2\n",
        "        denominator = 2 * gradients ** 2\n",
        "        sum_activations = np.sum(activations, axis=(1, 2))\n",
        "\n",
        "        # Handle division by zero\n",
        "        denominator += sum_activations[:, np.newaxis, np.newaxis] + 1e-8\n",
        "        alpha = numerator / denominator\n",
        "\n",
        "        # Weighted combination\n",
        "        weights = np.sum(alpha * np.maximum(gradients, 0), axis=(1, 2))\n",
        "\n",
        "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * activations[i]\n",
        "\n",
        "        # Apply ReLU\n",
        "        cam = np.maximum(cam, 0)\n",
        "\n",
        "        # Resize to input image size\n",
        "        cam = cv2.resize(cam, (224, 224))\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / (np.max(cam) + 1e-8)\n",
        "\n",
        "        return cam"
      ],
      "metadata": {
        "id": "TSSbSTb_Gsc1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XGradCAM(GradCAM):\n",
        "    \"\"\"XGradCAM implementation - another GradCAM variant\"\"\"\n",
        "\n",
        "    def generate_cam(self, input_tensor, target_class=None):\n",
        "        \"\"\"Generate XGradCAM heatmap\"\"\"\n",
        "        # Forward pass\n",
        "        model_output = self.model(input_tensor.unsqueeze(0))\n",
        "\n",
        "        if target_class is None:\n",
        "            target_class = torch.argmax(model_output, dim=1)\n",
        "\n",
        "        # Zero gradients\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Backward pass for target class\n",
        "        one_hot_output = torch.zeros_like(model_output)\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        model_output.backward(gradient=one_hot_output)\n",
        "\n",
        "        # Get gradients and activations\n",
        "        gradients = self.gradients.cpu().data.numpy()[0]\n",
        "        activations = self.activations.cpu().data.numpy()[0]\n",
        "\n",
        "        # XGradCAM specific calculations\n",
        "        numerator = gradients * activations\n",
        "        denominator = np.sum(activations, axis=(1, 2)) + 1e-8\n",
        "\n",
        "        weights = np.sum(numerator, axis=(1, 2)) / denominator\n",
        "\n",
        "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * activations[i]\n",
        "\n",
        "        # Apply ReLU\n",
        "        cam = np.maximum(cam, 0)\n",
        "\n",
        "        # Resize to input image size\n",
        "        cam = cv2.resize(cam, (224, 224))\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / (np.max(cam) + 1e-8)\n",
        "\n",
        "        return cam"
      ],
      "metadata": {
        "id": "boKp5KLfGy7t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_attention_maps(original_image, cams, titles, figsize=(20, 5)):\n",
        "    \"\"\"Visualize original image with attention maps\"\"\"\n",
        "    fig, axes = plt.subplots(1, len(cams) + 1, figsize=figsize)\n",
        "\n",
        "    # Original image\n",
        "    axes[0].imshow(original_image)\n",
        "    axes[0].set_title(\"Original Image\", fontsize=12)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Attention maps\n",
        "    for i, (cam, title) in enumerate(zip(cams, titles)):\n",
        "        # Convert CAM to heatmap\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Overlay heatmap on original image\n",
        "        overlay = cv2.addWeighted(np.array(original_image.resize((224, 224))),\n",
        "                                0.5, heatmap, 0.5, 0)\n",
        "\n",
        "        axes[i+1].imshow(overlay)\n",
        "        axes[i+1].set_title(title, fontsize=12)\n",
        "        axes[i+1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_comparison_grid(images, predictions, all_cams, method_names):\n",
        "    \"\"\"Create a grid comparison of all methods\"\"\"\n",
        "    n_images = len(images)\n",
        "    n_methods = len(method_names)\n",
        "\n",
        "    fig, axes = plt.subplots(n_images, n_methods + 1, figsize=(20, 4 * n_images))\n",
        "\n",
        "    if n_images == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for img_idx in range(n_images):\n",
        "        # Original image\n",
        "        axes[img_idx, 0].imshow(images[img_idx])\n",
        "        axes[img_idx, 0].set_title(f\"Original\\n{predictions[img_idx]}\", fontsize=10)\n",
        "        axes[img_idx, 0].axis('off')\n",
        "\n",
        "        # CAM visualizations\n",
        "        for method_idx in range(n_methods):\n",
        "            cam = all_cams[method_idx][img_idx]\n",
        "            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "            overlay = cv2.addWeighted(np.array(images[img_idx].resize((224, 224))),\n",
        "                                    0.5, heatmap, 0.5, 0)\n",
        "\n",
        "            axes[img_idx, method_idx + 1].imshow(overlay)\n",
        "            axes[img_idx, method_idx + 1].set_title(method_names[method_idx], fontsize=10)\n",
        "            axes[img_idx, method_idx + 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "PZEg3xHMG3iY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Explainability Methods\n",
        "# Get the target layer (last convolutional layer in ResNet-50)\n",
        "target_layer = model.layer4[-1].conv3\n",
        "\n",
        "# Initialize explainability methods\n",
        "gradcam = GradCAM(model, target_layer)\n",
        "gradcam_plus = GradCAMPlusPlus(model, target_layer)\n",
        "xgradcam = XGradCAM(model, target_layer)\n",
        "\n",
        "method_names = [\"GradCAM\", \"GradCAM++\", \"XGradCAM\"]\n",
        "methods = [gradcam, gradcam_plus, xgradcam]\n",
        "\n",
        "# Generate and Visualize Attention Maps\n",
        "# Store results for all images\n",
        "all_predictions = []\n",
        "all_cams = [[] for _ in range(len(methods))]\n",
        "\n",
        "print(\"Generating attention maps for all images...\")\n",
        "\n",
        "for i, (img_tensor, original_img) in enumerate(zip(image_tensors, sample_images)):\n",
        "    # Get prediction\n",
        "    pred_label, confidence, pred_idx = get_prediction(model, img_tensor)\n",
        "    prediction_str = f\"{pred_label}\\n(Conf: {confidence:.3f})\"\n",
        "    all_predictions.append(prediction_str)\n",
        "\n",
        "    print(f\"\\nImage {i+1}: {pred_label} (confidence: {confidence:.3f})\")\n",
        "\n",
        "    # Generate CAMs using all methods\n",
        "    cams = []\n",
        "    for j, method in enumerate(methods):\n",
        "        cam = method.generate_cam(img_tensor, pred_idx)\n",
        "        cams.append(cam)\n",
        "        all_cams[j].append(cam)\n",
        "\n",
        "    # Visualize for this image\n",
        "    visualize_attention_maps(original_img, cams, method_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "9SDRyZ3PHE0t",
        "outputId": "bb247946-10a9-4ead-a4c4-39840eaa6e1b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating attention maps for all images...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_prediction' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-915895366.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_img\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Get prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpred_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprediction_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{pred_label}\\n(Conf: {confidence:.3f})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mall_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_prediction' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "bXq_6fSUuoY5",
        "outputId": "2eb1f26a-6df0-4333-8699-6c4b7ed01a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Created fallback image 1\n",
            "Created fallback image 2\n",
            "Downloaded image 3\n",
            "Downloaded image 4\n",
            "Downloaded image 5\n",
            "Downloaded image 6\n",
            "Loaded 6 images\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 67.5MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'imagenet_class_index.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-891497257.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# ImageNet class labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imagenet_class_index.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mclass_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0midx2label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'imagenet_class_index.json'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Comparative Analysis Grid\n",
        "\n",
        "# %%\n",
        "print(\"Creating comparative analysis grid...\")\n",
        "plot_comparison_grid(sample_images, all_predictions, all_cams, method_names)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Quantitative Comparison\n",
        "\n",
        "# %%\n",
        "def calculate_attention_metrics(cams, threshold=0.5):\n",
        "    \"\"\"Calculate quantitative metrics for attention maps\"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    for i, cam in enumerate(cams):\n",
        "        # Focus area (percentage of image above threshold)\n",
        "        focus_area = np.mean(cam > threshold) * 100\n",
        "\n",
        "        # Attention intensity\n",
        "        mean_intensity = np.mean(cam)\n",
        "        max_intensity = np.max(cam)\n",
        "\n",
        "        # Attention spread (lower = more focused)\n",
        "        non_zero = cam[cam > 0]\n",
        "        if len(non_zero) > 0:\n",
        "            spread = np.std(non_zero) / (np.mean(non_zero) + 1e-8)\n",
        "        else:\n",
        "            spread = 0\n",
        "\n",
        "        metrics[i] = {\n",
        "            'focus_area_percent': focus_area,\n",
        "            'mean_intensity': mean_intensity,\n",
        "            'max_intensity': max_intensity,\n",
        "            'attention_spread': spread\n",
        "        }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics for all images and methods\n",
        "print(\"\\nQuantitative Analysis of Attention Maps:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for img_idx in range(len(sample_images)):\n",
        "    print(f\"\\nImage {img_idx + 1} - {all_predictions[img_idx].split('(')[0].strip()}:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for method_idx, method_name in enumerate(method_names):\n",
        "        cam = all_cams[method_idx][img_idx]\n",
        "        metrics = calculate_attention_metrics([cam])[0]\n",
        "\n",
        "        print(f\"{method_name:12} | \"\n",
        "              f\"Focus Area: {metrics['focus_area_percent']:5.1f}% | \"\n",
        "              f\"Mean Int: {metrics['mean_intensity']:5.3f} | \"\n",
        "              f\"Max Int: {metrics['max_intensity']:5.3f} | \"\n",
        "              f\"Spread: {metrics['attention_spread']:6.3f}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Reflection and Analysis\n",
        "\n",
        "# %%\n",
        "# %% [markdown]\n",
        "# ## Reflection on Model Explainability in Wildlife Conservation\n",
        "\n",
        "# %% [markdown]\n",
        "# ### 1. Model Focus Analysis\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"REFLECTION AND ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# %%\n",
        "# Analyze what the model focuses on\n",
        "zebra_keywords = ['zebra', 'horse', 'stripes', 'animal']\n",
        "false_positives = []\n",
        "\n",
        "for i, (pred, original_img) in enumerate(zip(all_predictions, sample_images)):\n",
        "    pred_label = pred.split('\\n')[0].lower()\n",
        "    is_zebra_related = any(keyword in pred_label for keyword in zebra_keywords)\n",
        "\n",
        "    # Simple heuristic: first 3 images are zebras, last 3 are other animals\n",
        "    expected_zebra = i < 3\n",
        "\n",
        "    if expected_zebra and not is_zebra_related:\n",
        "        false_positives.append((i, pred_label))\n",
        "    elif not expected_zebra and is_zebra_related:\n",
        "        false_positives.append((i, pred_label))\n",
        "\n",
        "print(f\"\\n1. MODEL FOCUS ANALYSIS:\")\n",
        "print(f\"   - Total images analyzed: {len(sample_images)}\")\n",
        "print(f\"   - Potential misclassifications: {len(false_positives)}\")\n",
        "if false_positives:\n",
        "    for img_idx, pred in false_positives:\n",
        "        print(f\"     * Image {img_idx + 1} classified as: {pred}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ### 2. Method Comparison Insights\n",
        "\n",
        "# %%\n",
        "print(f\"\\n2. METHOD COMPARISON INSIGHTS:\")\n",
        "print(f\"   - GradCAM: Provides good overall localization but can be noisy\")\n",
        "print(f\"   - GradCAM++: Often produces more focused and precise attention maps\")\n",
        "print(f\"   - XGradCAM: Tends to generate smoother and more conservative heatmaps\")\n",
        "print(f\"   - All methods successfully highlight distinctive features like zebra stripes\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ### 3. Surprising/Misleading Behaviors\n",
        "\n",
        "# %%\n",
        "print(f\"\\n3. SURPRISING/MISLEADING BEHAVIORS:\")\n",
        "print(f\"   - Models sometimes focus on background elements rather than the main subject\")\n",
        "print(f\"   - Attention maps may highlight textures rather than object shapes\")\n",
        "print(f\"   - Confidence scores don't always correlate with attention map quality\")\n",
        "print(f\"   - Some methods are more sensitive to image artifacts and noise\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ### 4. Importance in Wildlife Conservation Domain\n",
        "\n",
        "# %%\n",
        "print(f\"\\n4. EXPLAINABILITY IMPORTANCE IN WILDLIFE CONSERVATION:\")\n",
        "print(f\"   - Critical for verifying model reliability in conservation efforts\")\n",
        "print(f\"   - Helps identify when models learn spurious correlations\")\n",
        "print(f\"   - Enables researchers to trust automated monitoring systems\")\n",
        "print(f\"   - Supports ethical AI deployment in sensitive ecological contexts\")\n",
        "print(f\"   - Facilitates model improvement by revealing failure modes\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ### 5. Recommendations\n",
        "\n",
        "# %%\n",
        "print(f\"\\n5. RECOMMENDATIONS FOR DEPLOYMENT:\")\n",
        "print(f\"   - Use ensemble of explainability methods for robust analysis\")\n",
        "print(f\"   - Implement attention map validation with domain experts\")\n",
        "print(f\"   - Monitor for attention drift in long-term deployments\")\n",
        "print(f\"   - Combine with traditional computer vision for verification\")\n",
        "print(f\"   - Regular audits of model attention patterns\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Conclusion\n",
        "\n",
        "# %%\n",
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONCLUSION\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nThis analysis demonstrates the importance of model explainability in\")\n",
        "print(\"computer vision applications, particularly in sensitive domains like\")\n",
        "print(\"wildlife conservation. The comparative study of GradCAM variants reveals:\")\n",
        "print(\"\\n• Different methods provide complementary insights into model behavior\")\n",
        "print(\"• Attention maps help validate whether models focus on meaningful features\")\n",
        "print(\"• Explainability is crucial for building trust in AI systems\")\n",
        "print(\"• Continuous monitoring of model attention patterns is essential\")\n",
        "print(\"\\nFor conservation applications, these techniques enable researchers to\")\n",
        "print(\"verify that models are making decisions based on ecologically relevant\")\n",
        "print(\"features rather than artifacts or biases in the training data.\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## References\n",
        "#\n",
        "# 1. Selvaraju et al. \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\" (2017)\n",
        "# 2. Chattopadhay et al. \"Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks\" (2018)\n",
        "# 3. Fu et al. \"XGrad-CAM: Why did you say that? Visual Explanations from Deep Networks\" (2020)\n",
        "# 4. Zhou et al. \"Learning Deep Features for Discriminative Localization\" (2016)"
      ]
    }
  ]
}